{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5255a66",
   "metadata": {},
   "source": [
    "#### NO CORRER CON RUN ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a2b602b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\Facultad\\AA\\aprendizaje_automatico\\TP2\n",
      "D:\\Facultad\\AA\\aprendizaje_automatico\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(os.getcwd())\n",
    "script_dir = Path().resolve().parent\n",
    "os.chdir(script_dir)\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0490740",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "dataset_root = \"TP2/dataset/guardar_datos\"\n",
    "dataset_path = Path(dataset_root)\n",
    "train_parquet_path = dataset_path / \"train_dataset.parquet\"\n",
    "valid_parquet_path = dataset_path / \"valid_dataset.parquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf30f80",
   "metadata": {},
   "source": [
    "## CORRER ESTA CELDA SOLO UNA VEZ\n",
    "# 50 GB WARNING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17d0d613b719b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train instances: 180625\n",
      "Validation instances: 31875\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import pandas as pd\n",
    "from TP2.tokenizer import dataset_to_tokens\n",
    "\n",
    "import os\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def process_single_batch(args):\n",
    "    \"\"\"Worker function that processes one batch.\"\"\"\n",
    "    batch_data, batch_idx, split_name, dataset_path = args\n",
    "    tokenized = dataset_to_tokens(batch_data)\n",
    "    output_path = os.path.join(dataset_path, f\"{split_name}_tokens_batch_{batch_idx}.parquet\")\n",
    "    tokenized.to_parquet(output_path)\n",
    "    return batch_idx\n",
    "\n",
    "\n",
    "def batch_tokenizer():\n",
    "    TOKENIZER_BATCH_SIZE = 8192\n",
    "\n",
    "    train_df = pd.read_parquet(train_parquet_path)\n",
    "    train_labels = train_df['X']\n",
    "    print(f\"Train instances: {len(train_labels)}\")\n",
    "    train_tasks = [\n",
    "        (train_labels[i:i + TOKENIZER_BATCH_SIZE], i // TOKENIZER_BATCH_SIZE, 'train', dataset_path)\n",
    "        for i in range(0, len(train_labels), TOKENIZER_BATCH_SIZE)\n",
    "    ]\n",
    "\n",
    "    valid_df = pd.read_parquet(valid_parquet_path)\n",
    "    valid_labels = valid_df['X']\n",
    "    print(f\"Validation instances: {len(valid_labels)}\")\n",
    "    valid_tasks = [\n",
    "        (valid_labels[i:i + TOKENIZER_BATCH_SIZE], i // TOKENIZER_BATCH_SIZE, 'valid', dataset_path)\n",
    "        for i in range(0, len(valid_labels), TOKENIZER_BATCH_SIZE)\n",
    "    ]\n",
    "\n",
    "    all_tasks = train_tasks + valid_tasks\n",
    "    with Pool(processes=6) as pool:\n",
    "        results = pool.map(process_single_batch, all_tasks)\n",
    "\n",
    "    print(f\"Completed {len(results)} batches using {cpu_count()} cores\")\n",
    "\n",
    "# yo no lo descargo porque ya lo tengo\n",
    "batch_tokenizer()\n",
    "exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92438125",
   "metadata": {},
   "outputs": [],
   "source": [
    "INSTANCE_COUNT = 20000\n",
    "BATCH_SIZE = 8192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db1360d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [02:00<00:00,  5.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4523 | F1 Punct Ini: 0.5931 | F1 Punct Fin: 0.5141 | F1 Cap: 0.5653\n",
      "Val   Loss: 0.3704 | F1 Punct Ini: 0.6596 | F1 Punct Fin: 0.5372 | F1 Cap: 0.6244\n",
      "\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 82/625 [00:22<02:31,  3.58it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 44\u001b[39m\n\u001b[32m     39\u001b[39m     model = nn.DataParallel(model)\n\u001b[32m     41\u001b[39m model.to(device)\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m history = \u001b[43mtraining_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m#torch.save(model.state_dict(), \"rnn_simple_model.pth\")\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Facultad\\AA\\aprendizaje_automatico\\TP2\\training.py:129\u001b[39m, in \u001b[36mtraining_loop\u001b[39m\u001b[34m(model, device, train_loader, val_loader, criterion, optimizer, num_epochs, name_prefix)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m     train_loss, f1_punct_ini_train, f1_punct_final_train, f1_cap_train = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    132\u001b[39m     val_loss, f1_punct_ini_val, f1_punct_final_val, f1_cap_val = validation(\n\u001b[32m    133\u001b[39m         model, device, val_loader, criterion, optimizer\n\u001b[32m    134\u001b[39m     )\n\u001b[32m    136\u001b[39m     train_f1 = (f1_punct_ini_train + f1_punct_final_train + f1_cap_train) / \u001b[32m3.0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Facultad\\AA\\aprendizaje_automatico\\TP2\\training.py:17\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, device, data_loader, criterion, optimizer)\u001b[39m\n\u001b[32m     14\u001b[39m punct_fin_pred, punct_fin_ground_truth = [], []\n\u001b[32m     15\u001b[39m cap_pred, cap_ground_truth = [], []\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43membedding_group\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpunt_inicial_group\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpunt_final_group\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcapitalizacion_group\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# train data loader (un batch)\u001b[39;49;00m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_loss\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.0\u001b[39;49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrue_punt_inicial_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrue_punt_final_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrue_capitalizacion_idx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43membedding_group\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpunt_inicial_group\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpunt_final_group\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcapitalizacion_group\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python312\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    711\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    714\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    763\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m764\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    765\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    766\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Facultad\\AA\\aprendizaje_automatico\\TP2\\dataloader.py:65\u001b[39m, in \u001b[36mTextDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     63\u001b[39m effective_idx = idx % \u001b[38;5;28mself\u001b[39m._batch_size\n\u001b[32m     64\u001b[39m effective_idx = \u001b[38;5;28mself\u001b[39m._effective_index_shuffle[effective_idx] \u001b[38;5;66;03m# mapear al idx efectivo si es que estamos haciendo shuffle (dentro de este batch, no entra todo el dataset en memoria)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m emb = torch.tensor(np.vstack(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minstances\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43meffective_idx\u001b[49m\u001b[43m]\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33membedding\u001b[39m\u001b[33m\"\u001b[39m]), dtype=torch.float32)\n\u001b[32m     66\u001b[39m punt_ini = torch.tensor([\u001b[38;5;28mself\u001b[39m.instances.loc[effective_idx][\u001b[33m\"\u001b[39m\u001b[33mpunt_inicial\u001b[39m\u001b[33m\"\u001b[39m].to_list()], dtype=torch.long)\n\u001b[32m     67\u001b[39m punt_fin = torch.tensor([\u001b[38;5;28mself\u001b[39m.instances.loc[effective_idx][\u001b[33m\"\u001b[39m\u001b[33mpunt_final\u001b[39m\u001b[33m\"\u001b[39m].to_list()], dtype=torch.long)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python312\\Lib\\site-packages\\pandas\\core\\indexing.py:1192\u001b[39m, in \u001b[36m_LocationIndexer.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   1190\u001b[39m maybe_callable = com.apply_if_callable(key, \u001b[38;5;28mself\u001b[39m.obj)\n\u001b[32m   1191\u001b[39m maybe_callable = \u001b[38;5;28mself\u001b[39m._check_deprecated_callable_usage(key, maybe_callable)\n\u001b[32m-> \u001b[39m\u001b[32m1192\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python312\\Lib\\site-packages\\pandas\\core\\indexing.py:1432\u001b[39m, in \u001b[36m_LocIndexer._getitem_axis\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1430\u001b[39m \u001b[38;5;66;03m# fall thru to straight lookup\u001b[39;00m\n\u001b[32m   1431\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_key(key, axis)\n\u001b[32m-> \u001b[39m\u001b[32m1432\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_label\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python312\\Lib\\site-packages\\pandas\\core\\indexing.py:1382\u001b[39m, in \u001b[36m_LocIndexer._get_label\u001b[39m\u001b[34m(self, label, axis)\u001b[39m\n\u001b[32m   1380\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_label\u001b[39m(\u001b[38;5;28mself\u001b[39m, label, axis: AxisInt):\n\u001b[32m   1381\u001b[39m     \u001b[38;5;66;03m# GH#5567 this will fail if the label is not present in the axis.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1382\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mxs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python312\\Lib\\site-packages\\pandas\\core\\generic.py:4315\u001b[39m, in \u001b[36mNDFrame.xs\u001b[39m\u001b[34m(self, key, axis, level, drop_level)\u001b[39m\n\u001b[32m   4312\u001b[39m     index = \u001b[38;5;28mself\u001b[39m.index\n\u001b[32m   4314\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(index, MultiIndex):\n\u001b[32m-> \u001b[39m\u001b[32m4315\u001b[39m     loc, new_index = \u001b[43mindex\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_loc_level\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   4316\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m drop_level:\n\u001b[32m   4317\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m lib.is_integer(loc):\n\u001b[32m   4318\u001b[39m             \u001b[38;5;66;03m# Slice index must be an integer or None\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\multi.py:3321\u001b[39m, in \u001b[36mMultiIndex._get_loc_level\u001b[39m\u001b[34m(self, key, level)\u001b[39m\n\u001b[32m   3318\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m indexer, \u001b[38;5;28mself\u001b[39m[indexer]\n\u001b[32m   3320\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3321\u001b[39m     result_index = \u001b[43mmaybe_mi_droplevels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3322\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[32m   3323\u001b[39m     result_index = \u001b[38;5;28mself\u001b[39m[indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\multi.py:3192\u001b[39m, in \u001b[36mMultiIndex._get_loc_level.<locals>.maybe_mi_droplevels\u001b[39m\u001b[34m(indexer, levels)\u001b[39m\n\u001b[32m   3189\u001b[39m new_index = \u001b[38;5;28mself\u001b[39m[indexer]\n\u001b[32m   3191\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(levels, reverse=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m3192\u001b[39m     new_index = \u001b[43mnew_index\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_drop_level_numbers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3194\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m new_index\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:2207\u001b[39m, in \u001b[36mIndex._drop_level_numbers\u001b[39m\u001b[34m(self, levnums)\u001b[39m\n\u001b[32m   2204\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2205\u001b[39m     \u001b[38;5;66;03m# set nan if needed\u001b[39;00m\n\u001b[32m   2206\u001b[39m     mask = new_codes[\u001b[32m0\u001b[39m] == -\u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m2207\u001b[39m     result = \u001b[43mnew_levels\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_codes\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2208\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m mask.any():\n\u001b[32m   2209\u001b[39m         result = result.putmask(mask, np.nan)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:1173\u001b[39m, in \u001b[36mIndex.take\u001b[39m\u001b[34m(self, indices, axis, allow_fill, fill_value, **kwargs)\u001b[39m\n\u001b[32m   1171\u001b[39m values = \u001b[38;5;28mself\u001b[39m._values\n\u001b[32m   1172\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(values, np.ndarray):\n\u001b[32m-> \u001b[39m\u001b[32m1173\u001b[39m     taken = \u001b[43malgos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1174\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_na_value\u001b[49m\n\u001b[32m   1175\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1176\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1177\u001b[39m     \u001b[38;5;66;03m# algos.take passes 'axis' keyword which not all EAs accept\u001b[39;00m\n\u001b[32m   1178\u001b[39m     taken = values.take(\n\u001b[32m   1179\u001b[39m         indices, allow_fill=allow_fill, fill_value=\u001b[38;5;28mself\u001b[39m._na_value\n\u001b[32m   1180\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python312\\Lib\\site-packages\\pandas\\core\\algorithms.py:1239\u001b[39m, in \u001b[36mtake\u001b[39m\u001b[34m(arr, indices, axis, allow_fill, fill_value)\u001b[39m\n\u001b[32m   1234\u001b[39m     result = take_nd(\n\u001b[32m   1235\u001b[39m         arr, indices, axis=axis, allow_fill=\u001b[38;5;28;01mTrue\u001b[39;00m, fill_value=fill_value\n\u001b[32m   1236\u001b[39m     )\n\u001b[32m   1237\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1238\u001b[39m     \u001b[38;5;66;03m# NumPy style\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1239\u001b[39m     result = \u001b[43marr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1240\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from TP2.models.RNN_LSTM import RNN_LSTM\n",
    "from TP2.dataloader import TextDataset\n",
    "from TP2.training import training_loop\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "train_dataset = TextDataset(\n",
    "    base_path= dataset_root,\n",
    "    parquet_file_subpath=\"train_tokens_batch_\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    instance_count=INSTANCE_COUNT#252875 #TODO arreglar que no coincide con el print!!!!!!!!!!!!!!!!!!!!!!!!!!!11 (creo que ya está, ver la celda cursed del notebook separar datos)\n",
    "    #instance_count=291346  # ajustar segun la cantidad de instancias en el dataset\n",
    ")\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, collate_fn=train_dataset.collate_fn) # NO HACER SHUFFLE!\n",
    "\n",
    "\n",
    "valid_dataset = TextDataset(\n",
    "    base_path= dataset_root,\n",
    "    parquet_file_subpath=\"valid_tokens_batch_\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    instance_count=INSTANCE_COUNT#44625 #TODO arreglar que no coincide con el print!!!!!!!!!!!!!!!!!!!!!!!!!!!11\n",
    "    #instance_count=51415  # ajustar segun la cantidad de instancias en el dataset\n",
    ")\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=256, collate_fn=valid_dataset.collate_fn) # NO HACER SHUFFLE! TODO explicar por qué no shuffle\n",
    "\n",
    "model = RNN_LSTM(embedding_dim=768, hidden_dim=256, num_layers=2, bidirectional=True)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(RANDOM_SEED)\n",
    "\n",
    "# Si hay multiples GPUs, usamos DataParallel para envolver el modelo y poder utilizarlas\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Usando\", torch.cuda.device_count(), \"GPUs\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "history = training_loop(model, device, train_dataloader, valid_dataloader, criterion, optimizer, num_epochs=20)\n",
    "\n",
    "\n",
    "#torch.save(model.state_dict(), \"rnn_simple_model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
